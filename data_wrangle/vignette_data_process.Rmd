---
title: "Data Processing to create Vignette Data"
author: "Min-Yang Lee, Bryce McManus"
date: "`r Sys.Date()`"
output:
  html_document:
    keep_md: yes
    toc: yes
    toc_float: yes
    code_folding: show
params:
  location:
    label: 'Location:'
    value: Woods_Hole
    input: select
    choices:
    - Woods_Hole
    - Seattle
  PreRelease: yes
---
<!---
YAML options: 
location --- NEFSC folks should set location=Woods_Hole. AK folks should set it to Seattle. This just controls where the data is read in from.

PreRelease -- controls the location of FishSET package. Set to FALSE to load without options. Set to TRUE to load from a particular lib.loc
--->
## Introduction 
 This is some data processing to set up an anonymized dataset for FishSET

<br><br>

## Library

```{r setup, echo=FALSE}

#library("tidyverse")
tidyverse_short <- c("dplyr",  "ggplot2", "magrittr", "tidyr") 
lapply(tidyverse_short, require, character.only = TRUE)
rm(tidyverse_short)
library(kableExtra) # table formatting 
library(sf) # converting geometry points to separate cols
library(here)
here()
set.seed(20220823)
```
<br><br>


## Data Import

```{r data_readin, eval=TRUE}
#For Woods_Hole users, read in the most recent data in data/main

if (params$location=="Woods_Hole"){
  # This code looks into /data/main and sets the vintage_string according to 
  # the most recent data
  datasets_list <- list.files(path = here("data", "main"), pattern = "final_product_lease")
  datasets_list <- gsub("final_product_lease_", "", datasets_list )
  datasets_list <- gsub(".Rds", "", datasets_list)
  datasets_list <- gsub(".csv" ,"", datasets_list)
  vintage_string <- max(datasets_list)
  rm(datasets_list)
  dat_filepath <- here("data", "main", paste0("final_product_lease_", vintage_string, ".Rds"))
} else if (params$location=="Seattle"){
  dat_filepath <- paste0("~/NE Scallops/data/updated/", 
                         "June 22/NE NW Collab/",
                         "final_product_lease_2022_05_25.Rds")
}

vignette_data <- readRDS(dat_filepath)


```


## Aggregate subtrips to trips

The input data ``final_product_lease`` contains subtrips, not trips. This code chunk contracts trips to a single row, applying subtrip attributes for the most valuable subtrip (``max(DOLLAR)``)to the entire trip.

```{r construct_trip_aggregates, echo=TRUE, eval=TRUE}
# aggregate DOLLAR, POUNDS, LANDED by TRIPID and filter by max(DOLLAR)
vignette_data <- 
  vignette_data %>%
  group_by(TRIPID) %>%
  mutate(Agg_DOLLAR = sum(DOLLAR), Agg_POUNDS = sum(POUNDS), Agg_LANDED = sum(LANDED), Agg_DOLLAR_ALL_SP_2020 = sum(DOLLAR_ALL_SP_2020), Agg_DOLLAR_2020=sum(DOLLAR_2020))%>% 
  filter(DOLLAR == max(DOLLAR)) %>% 
  select(-DOLLAR, -POUNDS, -LANDED,-DOLLAR_ALL_SP_2020, -DOLLAR_2020) %>% 
  rename(DOLLAR = "Agg_DOLLAR", POUNDS = "Agg_POUNDS", LANDED = "Agg_LANDED", DOLLAR_ALL_SP_2020 = "Agg_DOLLAR_ALL_SP_2020", DOLLAR_2020 = "Agg_DOLLAR_2020") %>% 
  ungroup()

## Test out 
sum(duplicated(vignette_data$TRIPID))
stopifnot(sum(duplicated(vignette_data$TRIPID))==0)
```

## Keep  trips

```{r subset_on_fleet, echo=TRUE, eval=TRUE}
vignette_data <- 
  vignette_data %>%
filter(`Plan Code` == "SES" & (`Program Code`=="SAA" |`Program Code`=="SCA"))  
```


<br><br>
Keep just a few observations. A target is 10,000. A random sample is quick and easy for descriptive analysis. Something more sophisticated is warranted for more a choice model. 

```{r obscure_permit_and_operator}
#by each permit number, assign a new identification number.  Go in some sort of random order.
vignette_data <- vignette_data %>%
  group_by(PERMIT.y) %>%
    dplyr::mutate(FAKE_VES_ID = cur_group_id()) %>%
  relocate(FAKE_VES_ID ) %>%
  arrange(FAKE_VES_ID) %>%
  ungroup() %>%
  group_by(OPERNUM) %>%
  dplyr::mutate(FAKE_OPER_ID = cur_group_id()) %>%
  ungroup() %>%
  select(-c(PERMIT.y,OPERNUM))
```


```{r obscure_tripid}
# Assign a random trip id
vignette_data <- vignette_data %>%
  group_by(TRIPID) %>%
    dplyr::mutate(FAKE_TRIPID = cur_group_id()) %>%
  ungroup() %>%
  select(-c(TRIPID))
```

```{r pick_top_ports }
vignette_data <- vignette_data %>%
  add_count(geoid)%>%
  filter(n>=1000)
```

Just keep data from a few years, to capture some of the flavor of cross-year variation.
```{r subset_years, eval=FALSE}
vignette_data<-vignette_data %>%
  filter(YEAR %in% c(2008,2009,2010,2011,2012,2013,2014,2015))
```




```{r subset_random, eval=FALSE}
# keep 10,000 observations. 
vignette_data<-vignette_data %>%
  mutate(random = sample(1000,1)) %>%
  relocate(random) %>%
  arrange(random,FAKE_VES_ID) %>%
  select(-random)
vignette_data<-vignette_data[1:10000,]
```



```{r subset_group}
# keep a fraction of observations. But do it group-wise
vignette_data<-vignette_data %>%
  group_by(FAKE_VES_ID)%>% 
  mutate(random = sample(1000,1)) %>%
  relocate(random) %>%
  arrange(random,FAKE_VES_ID) %>%
  select(-random)
vignette_data<-vignette_data[1:10000,]
```


```{r obscure_locations, eval=FALSE}
#Add some noise to the lats and lons
vignette_data$shift1<-runif(nrow(vignette_data),.90,1.1)
vignette_data$shift2<-runif(nrow(vignette_data),.90,1.1)
vignette_data <- vignette_data %>%
    mutate(DDLAT=DDLAT*shift1,
           DDLON=DDLON*shift2) %>%
  select(-c(shift1,shift2))
```


```{r obscure_values}
# Add 20% noise to the LAT and LON data. This means that if you do a spatial join to the ten minute squares or to the wind areas, you will not match.  You may also match to land
#Compute the price. Add 20% noise to the landings and then update value.

vignette_data$shift3<-runif(nrow(vignette_data),.90,1.1)

vignette_data <- vignette_data %>%
    mutate(price=DOLLAR/LANDED,
           price2020=DOLLAR_2020/LANDED,
           LANDED=LANDED*shift3,
           DOLLAR=LANDED*price,
           DOLLAR_2020=LANDED*price2020,
           DOLLAR_ALL_SP_2020=DOLLAR_ALL_SP_2020*shift3) %>%
  select(-c(shift3))
```

```{r drop_extra_columns}
drop.cols <- c('OPERATOR', 'IMGID','SECGEARFISH', 'TRIP_ID', 'DEALNUM', 'state_fips', 'ftpt', 'GC', 'LA', 'DB_LANDING_YEAR', 'OBSERVED_COST_DUMMY', 'TRIP_COST_2020_DOL', 'NSUBTRIP', 'CREW','value','VTR_PORTNUM', 'VTR_PORT','VTR_STATE', 'DOLLAR_ALL_SP','Area Identifier', 'price', 'Date', 'Time', 'n', 'hours', 'YEAR','price2020')
vignette_data<- vignette_data %>%
  dplyr::select(-one_of(drop.cols))

```
```{r data_renameing}
vignette_data <- vignette_data %>%
  rename(PERMIT= FAKE_VES_ID,
         OPERNUM=FAKE_OPER_ID,
        TRIPID=FAKE_TRIPID)


vignette_data<-vignette_data %>%
  relocate(any_of(c("PERMIT", "TRIPID","DATE_TRIP","scallop_fishing_year","OPERNUM") ))
```


```{r data_saving, eval=TRUE}
if (params$location=="Woods_Hole"){
  vignette_out <- here("data", "main", paste0("scallop_vignette_", vintage_string, ".Rds"))
} else if (params$location=="Seattle"){
  vignette_out <- paste0("~/NE Scallops/data/updated/", 
                         "June 22/NE NW Collab/",
                         "scallop_vignette_", vintage_string,".Rds")
}


saveRDS(vignette_data, file=vignette_out)

```







